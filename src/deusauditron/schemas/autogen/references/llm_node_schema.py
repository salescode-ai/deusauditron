# generated by datamodel-codegen:
#   filename:  references/llm_node.schema.json

from __future__ import annotations

from enum import Enum
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, ConfigDict, PositiveFloat

from . import eval_rule_schema, filler_schema, intent_action_schema, model_schema
from .base_node_schema import BaseNodeConfig


class GlobalCodeExecutionStage(Enum):
	with_llm = "with_llm"
	before_llm = "before_llm"
	after_llm = "after_llm"


class ExecutionStage(Enum):
	with_llm = "with_llm"
	before_llm = "before_llm"
	after_llm = "after_llm"


class CodePath(BaseModel):
	model_config = ConfigDict(
		extra="forbid",
	)
	node: str
	execution_stage: ExecutionStage


class Paths(BaseModel):
	intent_based: Optional[Dict[str, intent_action_schema.IntentAction]] = None
	code_paths: Optional[List[CodePath]] = None
	rag_paths: Optional[List[str]] = None


class LLMNodeConfig(BaseNodeConfig):
	structured_output: Optional[bool] = False
	last_n_turns_llm: int
	last_n_turns_intent: int
	system_prompt: str
	overridden_global_prompt: Optional[str] = None
	next_node: Optional[str] = None
	global_code_execution_stage: Optional[GlobalCodeExecutionStage] = None
	input: Optional[Dict[str, Any]] = None
	paths: Optional[Paths] = None
	model: model_schema.ModelConfig
	filler: Optional[filler_schema.FillerConfig] = None
	timeout: Optional[PositiveFloat] = None
	evaluation_rules: Optional[List[eval_rule_schema.EvalRule]] = None
